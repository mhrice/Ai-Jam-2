{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhrice/Ai-Jam-2/blob/master/projects/voice_conversion/Local_Youtube_DL_%2B_Voice_conversion_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Voice Conversion Synthesizer\n",
        "\n",
        "**What is voice conversion?**\n",
        "\n",
        "Voice conversion is a process that involves changing the characteristics of one person's voice to sound like another person's voice. \n",
        "\n",
        "**Features of this system:**\n",
        "\n",
        "Depending on the audio you provide, the converted voice can:\n",
        "\n",
        "- Speak with different expressivities and emotions (neutral, normal, crying, angry, screaming) as well as in any language.\n",
        "- Speak fluently and naturally for long periods of time.\n",
        "- Singing in any language and with multiple vocal techniques.\n",
        "\n",
        "**What do I need to convert a voice?**\n",
        "\n",
        "You need a model of that voice previously trained with at least 10 minutes of audios. The more audios and better quality they are, the better the model will be.\n",
        "\n",
        "Follow the instructions in this notebook carefully and please be patient."
      ],
      "metadata": {
        "id": "MP5rRkbTpnG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mux_wwBggJKB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ↓ 1. Press play on the cell to download and install the necessary software.\n",
        "\n",
        "#@markdown #### (_This may take a few minutes_)\n",
        "\n",
        "!pip install gTTS\n",
        "\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "!pip install pydub\n",
        "\n",
        "!pip install wget\n",
        "\n",
        "from IPython.display import clear_output \n",
        "from google.colab import files \n",
        "import os\n",
        "\n",
        "!rm -rf /content/sample_data\n",
        "\n",
        "!git clone https://github.com/Mixomo/diff-svc.git\n",
        " \n",
        "%cd /content/diff-svc\n",
        "print('Installing torch')\n",
        "#!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install yt_dlp\n",
        "!pip install -r requirements_short.txt\n",
        "!pip install tensorboard<2.9,>=2.8\n",
        "!pip install -r requirements_short.txt\n",
        "!pip install tensorboard<2.9,>=2.8\n",
        "%reload_ext tensorboard\n",
        "print('Downloading pretrained models')\n",
        "%cd \"/content/\"\n",
        "%mkdir -p /content/diff-svc/checkpoints/\n",
        "!gdown --id 167Q2iTsf6cROH_IFnXbkbSdSK1q4VTbg -O checkpoints.zip\n",
        "!unzip /content/checkpoints.zip -d /content/diff-svc/\n",
        "!gdown --id 17H40ZgJ1Me5CtEAFSolZGAK8YCmifrAh\n",
        "!unzip /content/nyaru.zip -d /content/diff-svc/checkpoints/\n",
        "\n",
        "!gdown 1v7IXf0o5NqSfI3z2Pm_T8-JZc9YEEGHw -O /content/diff-svc/checkpoints/nsf_hifigan_finetune_20221211.zip\n",
        "!gdown 1-DjQo-pbRjuUQ5WI7IVuOCwqGuUx-jmA -O /content/diff-svc/checkpoints/nsf_hifigan_20221211.zip\n",
        "\n",
        "!unzip /content/diff-svc/checkpoints/nsf_hifigan_20221211.zip -d /content/diff-svc/checkpoints\n",
        "!unzip /content/diff-svc/checkpoints/nsf_hifigan_finetune_20221211.zip -d /content/diff-svc/checkpoints\n",
        "\n",
        "print(\"All set ✔️\")\n",
        "    #@markdown ___"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Youtube Download and Demucs Class\n",
        "from google.colab import files\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
        "from torchaudio.transforms import Fade\n",
        "import os # dope edit\n",
        "from urllib.parse import parse_qs, urlparse\n",
        "import yt_dlp\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class DownloadAndSplit:\n",
        "    def __init__(self, sample_rate: int = 48000, fade_overlap: float = 0.1):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.fade_overlap = fade_overlap\n",
        "        print(\"Loading model...\")\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
        "\n",
        "        sample_rate = bundle.sample_rate\n",
        "        self.model = bundle.get_model()\n",
        "        self.model.to(self.device)\n",
        "        self.root = Path(\"./downloads\")\n",
        "        self.root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def process(self, url: str, skip_download: bool = False):\n",
        "        root = Path(\"./downloads\")\n",
        "        if not skip_download:\n",
        "            print(\"Downloading song...\")\n",
        "            youtube_id = self.youtube_url_to_id(url)\n",
        "            path_to_download = root / youtube_id\n",
        "            path_to_download.mkdir(exist_ok=True)\n",
        "            output_file_path = path_to_download / \"output.wav\"\n",
        "            self.download_youtube(url, youtube_id, path_to_download)\n",
        "        else:\n",
        "            output_file_path = Path(url)\n",
        "            youtube_id = output_file_path.stem\n",
        "            path_to_download = root / output_file_path.stem\n",
        "            path_to_download.mkdir(exist_ok=True)\n",
        "\n",
        "        print(\"Separating...\")\n",
        "        mixture, sr = torchaudio.load(output_file_path, sample_rate)\n",
        "        mixture = torchaudio.functional.resample(mixture, sr, self.sample_rate)\n",
        "        sources = self.separate(mixture, model=self.model, device=self.device)\n",
        "        vocals = sources[\"vocals\"]\n",
        "        torchaudio.save(\n",
        "            f\"{path_to_download}/vocals.wav\",\n",
        "            vocals.cpu(),\n",
        "            sample_rate=self.sample_rate,\n",
        "        )\n",
        "        others = torch.zeros_like(vocals)\n",
        "        for source in sources:\n",
        "            if source != \"vocals\":\n",
        "                others += sources[source]\n",
        "        torchaudio.save(\n",
        "            f\"{path_to_download}/others.wav\",\n",
        "            others.cpu(),\n",
        "            sample_rate=self.sample_rate,\n",
        "        )\n",
        "\n",
        "        return vocals, others, youtube_id\n",
        "\n",
        "    def download_youtube(self, url: str, youtube_id: str, output_path: str):\n",
        "        options = {\n",
        "            \"format\": \"bestaudio/best\",\n",
        "            \"postprocessors\": [\n",
        "                {\n",
        "                    \"key\": \"FFmpegExtractAudio\",\n",
        "                    \"preferredcodec\": \"wav\",\n",
        "                    \"preferredquality\": \"192\",\n",
        "                }\n",
        "            ],\n",
        "            \"outtmpl\": os.path.join(output_path, f\"output.%(ext)s\"),\n",
        "        }\n",
        "        with yt_dlp.YoutubeDL(options) as youtube_dl:\n",
        "            youtube_dl.download([url])\n",
        "\n",
        "    def youtube_url_to_id(self, url: str) -> str:\n",
        "        url_data = urlparse(url)\n",
        "        query = parse_qs(url_data.query)\n",
        "        video_id = query[\"v\"][0]\n",
        "        return video_id\n",
        "\n",
        "    def separate(\n",
        "        self,\n",
        "        mix: torch.Tensor,\n",
        "        model: torch.nn.Module,\n",
        "        device: torch.device,\n",
        "        segment: int = 10,\n",
        "        overlap: float = 0.1,\n",
        "        overlap_frames: int = 0.1,\n",
        "    ):\n",
        "        mix = mix.unsqueeze(0).to(self.device)  # Add batch dimension\n",
        "        batch, channels, length = mix.shape\n",
        "        chunk_len = int(self.sample_rate * segment * (1 + overlap))\n",
        "        start = 0\n",
        "        end = chunk_len\n",
        "        overlap_frames = overlap * self.sample_rate\n",
        "        fade = Fade(\n",
        "            fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\"\n",
        "        )\n",
        "        ref = mix.mean(0)\n",
        "        mix = (mix - ref.mean()) / ref.std()  # normalization\n",
        "\n",
        "        sources = torch.zeros(\n",
        "            batch, len(model.sources), channels, length, device=device\n",
        "        )\n",
        "\n",
        "        while start < length - overlap_frames:\n",
        "            chunk = mix[:, :, start:end]\n",
        "            with torch.no_grad():\n",
        "                out = model.forward(chunk)\n",
        "            out = fade(out)\n",
        "            sources[:, :, :, start:end] += out\n",
        "            if start == 0:\n",
        "                fade.fade_in_len = int(overlap_frames)\n",
        "                start += int(chunk_len - overlap_frames)\n",
        "            else:\n",
        "                start += chunk_len\n",
        "            end += chunk_len\n",
        "            if end >= length:\n",
        "                fade.fade_out_len = 0\n",
        "        sources = sources * ref.std() + ref.mean()\n",
        "        sources_list = model.sources\n",
        "        sources = sources.squeeze(0)  # Drop batch dimension\n",
        "        sources = list(sources)\n",
        "        dict_sources = dict(zip(sources_list, sources))\n",
        "        return dict_sources\n"
      ],
      "metadata": {
        "id": "cFgyeRV_kpsd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Enter in the corresponding fields the Google Drive URLs (**VOICE MODEL and MODEL CONFIG**). make sure they are in \"any user with the link\" mode.\n",
        "\n",
        "#@markdown You can find some models in the \"model\" channel of the Diff-SVC discord\n",
        "\n",
        "VOICE_MODEL = \"https://drive.google.com/file/d/141YeMJFt-q-u1dDOoKvD_SvyJbzWC35N/\" #@param {type: \"string\"}\n",
        "\n",
        "MODEL_CONFIG = \"https://drive.google.com/file/d/1O6vu688nCFWeQlOB5UWKA0VEsB_9yzDc/\" #@param {type: \"string\"}\n",
        "\n",
        "# Extract the file ID from the URL\n",
        "VOICE_MODEL_ID = VOICE_MODEL.split(\"/\")[-2]\n",
        "MODEL_CONFIG_ID = MODEL_CONFIG.split(\"/\")[-2]\n",
        "\n",
        "# Download the file using gdown\n",
        "!gdown https://drive.google.com/uc?id=$VOICE_MODEL_ID -O last_checkpoint.ckpt\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=$MODEL_CONFIG_ID -O config.yaml\n",
        "\n",
        "print(\"All set ✔️\")\n",
        "\n",
        "#@markdown ___"
      ],
      "metadata": {
        "id": "msxadNKohIdm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and Split\n",
        "\n",
        "\n",
        "\n",
        "sample_rate = 44100\n",
        "# Process Youtube link\n",
        "song_link = \"/content/senoritapitchedup.mp3\" #@param {type: \"string\"}\n",
        "use_local = True #@param {type: \"boolean\"}\n",
        "ds = DownloadAndSplit(sample_rate=sample_rate, )\n",
        "vocals, others, youtube_id = ds.process(song_link, skip_download=use_local)\n",
        "filename = \"./downloads/vocals.wav\""
      ],
      "metadata": {
        "id": "lWHh0C-ik7Ni",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Workaround for numba/numpy issue. Please restart runtime after running this cell\n",
        "!pip install numpy==1.23.5 \n",
        "!pip install librosa==0.9.1\n",
        "!pip install numba==0.56.4"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NyOGriFqMWNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ↓ 3. Load the model.\n",
        "\n",
        "\n",
        "%cd \"/content/diff-svc/\"\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH']='.'\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "\n",
        "from utils.hparams import hparams\n",
        "from preprocessing.data_gen_utils import get_pitch_parselmouth,get_pitch_crepe\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import utils\n",
        "import librosa\n",
        "import torchcrepe\n",
        "from infer import *\n",
        "import logging\n",
        "from infer_tools.infer_tool import *\n",
        "\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "\n",
        "# 工程文件夹名，训练时用的那个\n",
        "project_name = \"any\" \n",
        "model_path = \"/content/last_checkpoint.ckpt\"\n",
        "config_path=\"/content/config.yaml\" \n",
        "hubert_gpu=True\n",
        "svc_model = Svc(project_name,config_path,hubert_gpu, model_path)\n",
        "print('model loaded')\n",
        "print(\"All set ✔️\")\n"
      ],
      "metadata": {
        "id": "dlM9RgYviRhy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Voice conversion settings and execution\n",
        "\n",
        "#@markdown #### (_This may take a few minutes_)\n",
        "\n",
        "#@markdown **NOTE: You must adjust the settings before executing the cell**.\n",
        "\n",
        "%cd \"/content/diff-svc/\"\n",
        "\n",
        "wav_fn = f\"/content/downloads/{youtube_id}/vocals.wav\"\n",
        "\n",
        "demoaudio, sr = librosa.load(wav_fn)\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``This parameter is used to match the audio you provided to the system to the target voice for correct conversion. It is handled by semitones, positive and negative numbers. A value of 12 is equivalent to one octave. ``\n",
        "#@markdown ___\n",
        "#@markdown #### ``If the conversion is from a male voice to a male voice, or from a female voice to a female voice, you may leave this value at 0. ``\n",
        "#@markdown ___\n",
        "#@markdown #### ``On the other hand, if the conversion is from a male voice to a female voice, we recommend increase the value until the converted voice sounds to your liking. Reference value: 12``.\n",
        "#@markdown ___\n",
        "#@markdown #### ``And if the conversion is made from a female voice to a male voice, lower the value (negative numbers) until the converted voice sounds to your liking. Reference value: -12.``\n",
        "#@markdown ___\n",
        "#@markdown #### ``You are free to experiment with intermediate, higher, or lower values until you achieve the desired result. it is trial and error``.\n",
        "#@markdown ___\n",
        "key = 0#@param {type: \"integer\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``This parameter is used to modify the steps of each generation / synthesis. the minimum value that gives a decent quality of the voice is 20. you can increase the value to try to improve the quality of the conversion, although it will take more time. a recommended value may be 50. ``\n",
        "\n",
        "pndm_speedup = 20 #@param {type: \"integer\"}\n",
        "\n",
        "wav_gen='/content/converted_audio.wav' \n",
        "\n",
        "add_noise_step = 500 \n",
        "\n",
        "thre = 0.05 \n",
        "\n",
        "use_crepe= False\n",
        "\n",
        "use_pe=False\n",
        "#@markdown ___\n",
        "#@markdown #### ``This parameter is used to use the MEL spectrogram of the incoming audio as a starting point for the conversion. Sometimes it gives better results, sometimes not. experimentation is recommended. ``\n",
        "use_gt_mel= False #@param {type: \"boolean\"}\n",
        "\n",
        "f0_tst, f0_pred, audio = run_clip(svc_model,file_path=wav_fn, key=key, acc=pndm_speedup, use_crepe=use_crepe, use_pe=use_pe, thre=thre,\n",
        "                                        use_gt_mel=use_gt_mel, add_noise_step=add_noise_step,project_name=project_name,out_path=wav_gen)\n",
        "\n",
        "print(\"All set ✔️\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DauP3lXimfS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ↓ 5. Play the audio with the converted voice.\n",
        "#@markdown If you don't like the result, change the settings in the above cell and run it again until you find the value you like.\n",
        "\n",
        "#@markdown **It may happen that with long audios (full songs for example), it may take a while to display the player and end up crashing Colab. In such cases we recommend that you directly download the audio to your computer with the cell below.**\n",
        "\n",
        "ipd.display(ipd.Audio(audio, rate=hparams['audio_sample_rate'], normalize=True))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZQMM6ctnmizz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ↓ 6. Combine and download the audio\n",
        "import torch.nn.functional as F\n",
        "import soundfile as sf\n",
        "from torchaudio.functional import resample\n",
        "from pydub import AudioSegment\n",
        "%cd /content/\n",
        "\n",
        "def get_track_pitch_shift(vocal_key):\n",
        "  while vocal_key < 0:\n",
        "    vocal_key += 12\n",
        "  if vocal_key % 12 <= 6:\n",
        "    ps = vocal_key\n",
        "  else:\n",
        "    ps = -(12 - vocal_key)\n",
        "  return ps\n",
        "\n",
        "#read wav file to an audio-segment\n",
        "converted_vocals, sr = torchaudio.load(wav_gen)\n",
        "converted_vocals = converted_vocals.to(others.device)\n",
        "converted_vocals = resample(converted_vocals, sr, sample_rate).squeeze(0)\n",
        "# Convert to stereo\n",
        "converted_vocals = torch.stack([converted_vocals, converted_vocals])\n",
        "\n",
        "# Pitch shift beat to match vocals\n",
        "ps = get_track_pitch_shift(key)\n",
        "track = torchaudio.functional.pitch_shift(others, sample_rate, n_steps=ps)\n",
        "\n",
        "# Pad the vocals to match the others\n",
        "diff = track.shape[1] - converted_vocals.shape[1]\n",
        "if diff > 0:\n",
        "    converted_vocals = F.pad(converted_vocals, (0, diff))\n",
        "elif diff < 0:\n",
        "    track = F.pad(track, (0, -diff))\n",
        "\n",
        "full_song = converted_vocals + track\n",
        "\n",
        "\n",
        "sf.write(\"/content/converted_vocals.wav\", converted_vocals.T.cpu(), sample_rate)\n",
        "sf.write(\"/content/track.wav\", track.T.cpu(), sample_rate)\n",
        "sf.write(\"/content/output.wav\", full_song.T.cpu(), sample_rate)\n",
        "\n",
        "\n",
        "song = AudioSegment.from_wav(\"/content/converted_vocals.wav\")\n",
        "song.export(\"vocals.mp3\", format=\"mp3\")\n",
        "files.download('/content/vocals.mp3')\n",
        "\n",
        "song = AudioSegment.from_wav(\"/content/track.wav\")\n",
        "song.export(\"track.mp3\", format=\"mp3\")\n",
        "files.download('/content/track.mp3')\n",
        "\n",
        "song = AudioSegment.from_wav(\"/content/output.wav\")\n",
        "song.export(\"output.mp3\", format=\"mp3\")\n",
        "files.download('/content/output.mp3') "
      ],
      "metadata": {
        "id": "VxRCkHMJ9ihy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ↓ 7. Delete the reference audio to re-generate or upload a new one.\n",
        "\n",
        "!rm -r /content/reference_audio.wav"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1uR3nJbRwG32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: To generate a new voice conversion, run step 4, step 5 (changing the settings) and step 6.\n",
        "_______________________\n",
        "*If in cell 5 and/or 6 there is an error, simply rerun them, or try uploading a shorter audio."
      ],
      "metadata": {
        "id": "0uj3dcHNDd5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you have finished using the system, go to Runtime Environment > Disconnect and delete runtime environment."
      ],
      "metadata": {
        "id": "zui2nnEyfGfW"
      }
    }
  ]
}